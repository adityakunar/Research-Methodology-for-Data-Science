---
title: "Research Question 1"
output: html_notebook
---

*Question - How much does transfer learning improve over typical non-transfer learning?*


### Importing libraries and Reading in the dataset.

```{r,echo=FALSE}
library(pander)
library(car)
library('ggplot2')
library('emmeans')
library('ggpubr')
data<- read.csv("../data/data.csv") #Read in the data in R.
```

## Understanding the dataset.
```{r,echo=FALSE}
table(data$model)# Count the data points for each model.
```
Here we see that we only have 21 points for the first scenario which involves no transfer learning. Whereas with transfer learning, there are many more data points.

Lets naively observe the means for the models.
```{r,echo=FALSE}
aggregate(score~model, data, mean) # We can look at the means for each model.
```
This gives the mean scores for each model. We can see that the MN model has the highest mean score whereas the B2 model has the lowest mean score. We can also start to see that transfer learning at least doesn't negatively impact the mean scores of the models. However we still need to determine if anything can be said about it postively impacting the scores of the models in our dataset. 
```{r,echo=FALSE}
aggregate(score~TeD, data,mean) # We can also look at the means for each test dataset. 
```    
Here we see the scores aggregated on the test datasets. We know that TeD1 to TeD4 are classification datasets and their mean scores are found to be higher than for the other tasks such as regression and recommendation. Infact we see the lowest performance for the recommendation task dataset i.e TeD5. We are beginning to realise that the *mean score of any model(transfer learning or not) might also be impacted by the test dataset as well*. 


### visualisations of model and dataset scores.
```{r,echo=FALSE}
plot(data$model, data$score, xlab = "Scores", ylab ="Models")
```
```{r,echo=FALSE}
plot(data$TeD, data$score, xlab ="Dataset", ylab ="Scores")
```
Here we show a visual representation for our arguments made before in the previous section.  


```{r,echo=FALSE}
# storing all the no transfer learning scores.
No_TL <- data$score[data$model=="B1"|data$model=="B2"|data$model=="B3"] 

# storing all the transfer learning scores.
TL <- data$score[data$model=="MN" | data$model=="M1"|data$model=="M2"|data$model=="M3"|data$model=="MF"|data$model=="S"]

#adding new factor corresponding to whether a model uses transfer learning or not.
data$TL <- with(data, ifelse(model=="B1"|model=="B2"|model=="B3","No_TL", "Yes_TL"))
data$TL <- factor(data$TL)
```


## Exploring interaction effects of Test Datasets.

### Visualising the interaction. 
```{r}
ggplot(data, aes(TL, score)) + geom_boxplot(notch = FALSE) + facet_wrap(~TeD)
```
From the above analysis we can see that the test dataset creates a great degree of variation in the differences between models with transfer learning and without. 

### Mutli Level Modelling

we wanted to perform a hierarchical analysis due to the fact that there was a clear dependency in observations obtained from the same test dataset. Therefore, we have used a multi-level model wherein the fixed effect is whether transfer learning was used or not and the random effect consists of random slopes for the difference in scenarios with respect to transfer learning being used and random intercepts for the different test datasets. 


```{r,echo=FALSE}
randomSlopeTL <- lme(score ~ TL, data = data, random = ~TL|TeD, method = "ML")
summary(randomSlopeTL)
```
 Based on this model, we see that the presence of information regarding the use of transfer learning(t=3.26,df=2988,p.<0.01) has a significant impact on the scores of our models. We see that there is on average, a positive deviation of .12(95% CI, 0.04 to 0.18) across the different datasets for when transfer learning is used as compared to when it's not used.


```{r, echo=FALSE}
intervals(randomSlopeTL,0.95)
```

From the results shown above it can be seen that there is a significant variation in the scores caused by differences in the test datasets. This can be verified by looking at the confidence intervals of the random effects and more specifically the standard deviation between the fitted intercepts for the different test datasets which is 0.21 (95% CI, 0.12 to 0.36).  Futhermore, we can also see that there is very little standard deviation between the slopes fitted on the basis of whether transfer learning was used for different test datasets suggesting that all datasets mostly had the same beneficial effect from the use of transfer learning. Lastly, we can see that the confidence intervals are quite broad, this is an indication that there is a great deal of uncertainity in our estimates and we believe this is due to the imbalance of data points for when transfer learning was used vs when it wasn't as shown in the very beginning of the analysis. 


### The use of EMMEANS.

For more clarity in our interpretation, we can also use the library emmeans.

```{r,echo=FALSE}
emmeans(m, pairwise~TL|TeD)
```    
Finally we have a few more significant results. We see that there are significant differences(p.<0.001) for the following datasets: TeD1, TeD3, TeD6 and TeD7. We can see that in all cases, the model scores are higher for when transfer learning was used with the highest impact on the regression dataset TeD6(difference of .28) followed by the classification dataset TeD1(difference of .20), the regression dataset TeD7 (0.17) and lastly, the classification dataset TeD3 (.12). 


