---
title: "Coursework Assignment B - 2020"
output:
  html_notebook: default
  html_document:
    df_print: paged
  pdf_document: default
---

### CS4125 Seminar Research Methodology for Data Science

#### By- Aditya Kunar (5074274), Sharwin Bobde (5011639), Pavel Hoogland (4450892)


# Research Question 1

## *Question - How much does transfer learning improve over typical non-transfer learning?*


```{r,echo=FALSE}
library(pander)
library(car)
library('ggplot2')
library('emmeans')
library('ggpubr')
data<- read.csv("../data/data.csv") #Read in the data in R.
```

## 1) Understanding the dataset.

#### a) Counts for the observations for each model. 
```{r,echo=FALSE}
table(data$model)# Count the data points for each model.
```
Here we see that we only have 21 points for the first scenario(B1,B2,B3) which involves no transfer learning. Whereas with transfer learning, there are many more data points.

#### b) Observing the mean scores for the models.
```{r,echo=FALSE}
aggregate(score~model, data, mean) # We can look at the means for each model.
```
We can see that the MN model has the highest mean score(.53) whereas the B2 model has the lowest mean score(0.32). We can also start to see that transfer learning at least doesn't negatively impact the mean scores of the models. However we still need to determine if anything can be said about it postively impacting the scores of the models in our dataset. 

#### c) Observing the means for the models.
```{r,echo=FALSE}
aggregate(score~TeD, data,mean) # We can also look at the means for each test dataset. 
```    
Here we see the scores aggregated on the test datasets. We know that TeD1 to TeD4 are classification datasets and their mean scores are found to be higher than for the other tasks such as regression and recommendation. Infact we see the lowest performance for the recommendation task dataset i.e TeD5 with a mean score of 0.05. We are beginning to realise that the *mean score of any model(transfer learning or not) might also be impacted by the test dataset as well*. 


#### d) Visualisations of model and dataset scores.
```{r,echo=FALSE}
plot(data$model, data$score, xlab = "Scores", ylab ="Models")
```

By looking at the box plots of scores aggregated on models, we can see that models M1, M2, M3 and MN have somewhat similar scores and the median scores of these models tend to be higher than for models B1,B2 and B3 for which transfer learning has not been used. We can also clearly see that the models for which transfer learning has been used have a larger spread in their scores. From this boxplot we can start to appreciate that models play a role in affecting the scores in our dataset.

```{r,echo=FALSE}
plot(data$TeD, data$score, xlab ="Dataset", ylab ="Scores")
```
 By looking at scores aggregated on test datasets, we can clearly see there is no homogeneity amongst the scores on different test datasets. We see that TeD1 has the highest median score and TeD5 has the lowest with a very small interquartile range as well. We also notice that TeD6 has quite a few outliers as compared to the other test datasets. But it is clear that test datasets do have an influence on the scores in our dataset.  
 
 

```{r,echo=FALSE}
# storing all the no transfer learning scores.
No_TL <- data$score[data$model=="B1"|data$model=="B2"|data$model=="B3"] 

# storing all the transfer learning scores.
TL <- data$score[data$model=="MN" | data$model=="M1"|data$model=="M2"|data$model=="M3"|data$model=="MF"|data$model=="S"]

#adding new factor corresponding to whether a model uses transfer learning or not.
data$TL <- with(data, ifelse(model=="B1"|model=="B2"|model=="B3","No_TL", "Yes_TL"))
data$TL <- factor(data$TL)
```


#### e) Visualising the interaction between model and test datasets. 
```{r,echo=false}
ggplot(data, aes(TL, score)) + geom_boxplot(notch = FALSE) + facet_wrap(~TeD)
```
From the above visualization, we can see that the test dataset creates a great degree of variation in the differences between models scores with transfer learning and without. We can also notice that except for TeD4 and TeD5, models with transfer learning
tend to do better on most other test datasets. 

## 2) Mutli Level Modelling

We wanted to perform a hierarchical analysis due to the fact that there was a clear dependency in observations obtained from the same test dataset. Therefore, we have used a multi-level model wherein the random effects comprise of random interecepts for the different test datasets and random slopes for the difference in scenarios between whether transfer learning was used or not used. The fixed effect of our model solely consists of a predictor variable indicating whether transfer learning was used or not  


```{r,echo=False}
randomSlopeTL <- lme(score ~ TL, data = data, random = ~TL|TeD, method = "ML")
summary(randomSlopeTL)
```
 Based on this model, we see that the presence of information regarding the use of transfer learning(t=3.26,df=2988,p.<0.01) has a significant impact on the scores of our models. We see that there is on average, a positive deviation of 0.12(95% CI, 0.04 to 0.18) across the different datasets for when transfer learning is used as compared to when it's not used.


```{r, echo=FALSE}
intervals(randomSlopeTL,0.95)
```

From the results shown above it can be seen that there is a significant variation in the scores caused by differences in the test datasets. This can be verified by looking at the confidence intervals of the random effects and more specifically the standard deviation between the fitted intercepts for the different test datasets which is 0.21 (95% CI, 0.12 to 0.36).  Futhermore, we can also see that the standard deviation between the slopes is 0.086 (95% CI, 0.04 to 0.16) is not too big which is fitted on the basis of whether transfer learning was used for different test datasets suggesting that the different test datasets mostly had a beneficial effect from the use of transfer learning. Lastly, we can see that the confidence intervals are quite broad, this is an indication that there is a great deal of uncertainity in our estimates and we believe this is due to the imbalance of data points for when transfer learning was used vs when it wasn't as shown in the very beginning of the analysis. 


### The use of EMMEANS.

For more clarity in our interpretation, we can also use the library emmeans.

```{r,echo=FALSE}
m<- lm(score~TeD*TL,data = data)
emmeans(m, pairwise~TL|TeD)
```    
Finally we have a few more significant results. We see that there are significant differences(p.<0.001) for the following datasets: TeD1, TeD3, TeD6 and TeD7. We can see that in all cases, the model scores are higher for when transfer learning was used with the highest impact on the regression dataset TeD6(difference of .28) followed by the classification dataset TeD1(difference of .20), the regression dataset TeD7 (0.17) and lastly, the classification dataset TeD3 (.12). 


